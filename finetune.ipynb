{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R6OG8uOYQHi",
        "outputId": "66ef4fb9-f1f6-410e-c0bb-b55d383edd84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GPT-2 with LoRA ---\n",
            "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n",
            "\n",
            "--- ViT with LoRA ---\n",
            "trainable params: 1,339,392 || all params: 87,728,640 || trainable%: 1.5267\n",
            "Path to dataset files: /kaggle/input/flickr-image-dataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config , BitsAndBytesConfig\n",
        "from transformers import ViTModel\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import GPT2Model\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "# Load Models and Tokenizer\n",
        "base_model = GPT2LMHeadModel.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,   # QLoRA requires 4-bit or 8-bit\n",
        ")\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "# ViT is not a language model, so we don't need to quantize it unless we're memory-constrained\n",
        "# For simplicity, let's load it normally on the GPU\n",
        "vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "\n",
        "# --- LoRA Configuration for GPT-2 ---\n",
        "lora_config1 = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    # Best target modules for GPT-2\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# --- LoRA Configuration for ViT ---\n",
        "# Note: ViT is not a Causal LM, so TaskType should not be set or set appropriately\n",
        "# for the task you are doing (e.g., image classification).\n",
        "# Since you're just adapting the base ViTModel, we can omit task_type.\n",
        "lora_config2 = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    # Best target modules for ViT\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# Wrap GPT-2 with QLoRA\n",
        "lora_gpt2_model = get_peft_model(base_model, lora_config1)\n",
        "print(\"--- GPT-2 with LoRA ---\")\n",
        "lora_gpt2_model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "# Wrap ViT with LoRA\n",
        "lora_vit_model = get_peft_model(vit, lora_config2)\n",
        "print(\"\\n--- ViT with LoRA ---\")\n",
        "lora_vit_model.print_trainable_parameters()\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"hsankesara/flickr-image-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1= pd.read_csv(r\"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\", delimiter='\\t', engine='python')\n"
      ],
      "metadata": {
        "id": "L0L2ExxLZCfT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  transform=transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Added normalization as a common practice\n",
        "    ])"
      ],
      "metadata": {
        "id": "77okUtUscjLC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlickrDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    self.annotations = csv_file\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
        "    image = io.imread(img_path)\n",
        "    txt=tokenizer(self.annotations.iloc[index, 1], return_tensors='pt')\n",
        "    txt=torch.tensor(txt)\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return (image, txt)\n",
        "data=FlickrDataset(df1,r\"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\",transform=transform)\n",
        "dataloader=DataLoader(data, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "bRUbBZLEbYgo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Img2GPT(nn.Module):\n",
        "    def __init__(self, gptt, vitt):\n",
        "        super(Img2GPT, self).__init__()\n",
        "        self.vit = vitt\n",
        "        self.gpt2 = gptt\n",
        "\n",
        "        vit_dim = self.vit.config.hidden_size\n",
        "        gpt_dim = self.gpt2.config.hidden_size\n",
        "\n",
        "        # project ViT outputs to GPT2 input size\n",
        "        self.proj = nn.Linear(vit_dim, gpt_dim)\n",
        "\n",
        "    def forward(self, images):\n",
        "        # ViT forward\n",
        "        vit_outputs = self.vit(images)\n",
        "        img_embeds = vit_outputs.last_hidden_state  # (batch, seq_len, vit_dim)\n",
        "\n",
        "        # Project to GPT2 dimension\n",
        "        img_embeds = self.proj(img_embeds)  # (batch, seq_len, gpt_dim)\n",
        "\n",
        "        # Feed into GPT2 as inputs_embeds\n",
        "        outputs = self.gpt2(inputs_embeds=img_embeds)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "UANEe1PffQME"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Img2GPT(lora_gpt2_model, lora_vit_model)\n",
        "def count_all_parameters(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "total, trainable = count_all_parameters(model)\n",
        "print(f\"Total parameters: {total}\")\n",
        "print(f\"Trainable parameters: {trainable}\")\n",
        "print(f\"Frozen parameters: {total - trainable}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9DQbcwRuzRm",
        "outputId": "91c67c14-1e2a-4d57-8ec5-d625edb25ea0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 171102720\n",
            "Trainable parameters: 3331584\n",
            "Frozen parameters: 167771136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m4tCxasuvNmj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
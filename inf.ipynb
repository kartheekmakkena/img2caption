{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "426b4a75be2f406ca149c2e12b27a7de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e091d61d96b4ac6b6f5acc0c335bcaf",
              "IPY_MODEL_a05da501d48e426e8ba2d3ac671a8fe1",
              "IPY_MODEL_cddcbb8a2fc8416b94e03c224ffc8f25"
            ],
            "layout": "IPY_MODEL_dc24988b9fc347259069ef1bf6d4b9ed"
          }
        },
        "3e091d61d96b4ac6b6f5acc0c335bcaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c92a3a9a0bde4ca98d67048020d75dd8",
            "placeholder": "​",
            "style": "IPY_MODEL_ca91380e115743458624cbec68b746aa",
            "value": "Epoch 1/3: 100%"
          }
        },
        "a05da501d48e426e8ba2d3ac671a8fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41f8e4154cd54f5287b3748d7d710bb3",
            "max": 7946,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2e4bea6ca6f4a2d9248489d04749c0a",
            "value": 7946
          }
        },
        "cddcbb8a2fc8416b94e03c224ffc8f25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59c6b616d9b140c2818fbd669bc5fd62",
            "placeholder": "​",
            "style": "IPY_MODEL_f504994f2fa04ef5a0f756053c116fd2",
            "value": " 7946/7946 [1:37:21&lt;00:00,  1.50it/s, loss=10]"
          }
        },
        "dc24988b9fc347259069ef1bf6d4b9ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c92a3a9a0bde4ca98d67048020d75dd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca91380e115743458624cbec68b746aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41f8e4154cd54f5287b3748d7d710bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2e4bea6ca6f4a2d9248489d04749c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59c6b616d9b140c2818fbd669bc5fd62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f504994f2fa04ef5a0f756053c116fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "067d5cc6bdba48869da5719fb0448e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f55002a5e33412eacb80ccfd529dc39",
              "IPY_MODEL_a877c6866d6943ab90ec612e4dd57155",
              "IPY_MODEL_2a3200735fc94e1a8afa5e86b24dfa98"
            ],
            "layout": "IPY_MODEL_6b2dc7f5852f42f4a285362e05f9472a"
          }
        },
        "3f55002a5e33412eacb80ccfd529dc39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_472dbca603b94f71877be10d08ccf1a9",
            "placeholder": "​",
            "style": "IPY_MODEL_58d238b69b0f4474baddf40c9de77327",
            "value": "Epoch 2/3: 100%"
          }
        },
        "a877c6866d6943ab90ec612e4dd57155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d35d209f16841ea91668c0ec54bfc33",
            "max": 7946,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16f1f2fc34c84b28a6e8ece967ad740e",
            "value": 7946
          }
        },
        "2a3200735fc94e1a8afa5e86b24dfa98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35fe8062bf394b9f93408b83ce2336c2",
            "placeholder": "​",
            "style": "IPY_MODEL_66da745008984733a479ce0bb7907c75",
            "value": " 7946/7946 [1:37:02&lt;00:00,  1.50it/s, loss=10.1]"
          }
        },
        "6b2dc7f5852f42f4a285362e05f9472a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "472dbca603b94f71877be10d08ccf1a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d238b69b0f4474baddf40c9de77327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d35d209f16841ea91668c0ec54bfc33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f1f2fc34c84b28a6e8ece967ad740e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35fe8062bf394b9f93408b83ce2336c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66da745008984733a479ce0bb7907c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f3afbac90ee4337b3d7e090af070e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9808c927ea7149d58c814a872cb3430d",
              "IPY_MODEL_b0f4d314987e4e7aa8c23aecd477b4d8",
              "IPY_MODEL_814c70e296324eac8987aaf148135327"
            ],
            "layout": "IPY_MODEL_aa087b0f25194804824cb804d10d7a9f"
          }
        },
        "9808c927ea7149d58c814a872cb3430d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a65a2715c0914c1b85fd2b0830ae9604",
            "placeholder": "​",
            "style": "IPY_MODEL_d0c1363c6c0c4810a09afdcd26fec5e8",
            "value": "Epoch 3/3: 100%"
          }
        },
        "b0f4d314987e4e7aa8c23aecd477b4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f70bd2f5621c41efaed46db6798b9aec",
            "max": 7946,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4158e947107744a7aebcdd7e7f5a4ac3",
            "value": 7946
          }
        },
        "814c70e296324eac8987aaf148135327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffce5b6a3eb949c88b35d1a5d64969d6",
            "placeholder": "​",
            "style": "IPY_MODEL_0648044715b7408e895135d7d690f248",
            "value": " 7946/7946 [1:36:57&lt;00:00,  1.51it/s, loss=9.88]"
          }
        },
        "aa087b0f25194804824cb804d10d7a9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a65a2715c0914c1b85fd2b0830ae9604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0c1363c6c0c4810a09afdcd26fec5e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f70bd2f5621c41efaed46db6798b9aec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4158e947107744a7aebcdd7e7f5a4ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ffce5b6a3eb949c88b35d1a5d64969d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0648044715b7408e895135d7d690f248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. SETUP: INSTALL LIBRARIES AND IMPORTS\n",
        "# ==============================================================================\n",
        "# Install required libraries in the Colab environment\n",
        "!pip install -q kagglehub transformers accelerate bitsandbytes peft tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import PIL.Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from transformers import (\n",
        "    ViTModel,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2TokenizerFast,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CONFIGURATION: CHOOSE DEVICE AND SET PARAMETERS\n",
        "# ==============================================================================\n",
        "# <<<<<<<<<<<<<<< CHOOSE YOUR DEVICE HERE >>>>>>>>>>>>>>>>>\n",
        "DEVICE_CHOICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "device = torch.device(DEVICE_CHOICE)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Model & Training Parameters ---\n",
        "GPT2_MODEL_NAME = \"gpt2\"\n",
        "VIT_MODEL_NAME = \"google/vit-base-patch16-224\"\n",
        "BATCH_SIZE = 20\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 5e-5\n",
        "MAX_TEXT_LENGTH = 128\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. MODEL AND TOKENIZER LOADING (WITH CONDITIONAL QUANTIZATION)\n",
        "# ==============================================================================\n",
        "print(\"Loading models and tokenizer...\")\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(GPT2_MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    print(\"Added [PAD] token to tokenizer.\")\n",
        "\n",
        "quantization_config = None\n",
        "if DEVICE_CHOICE == \"cuda\":\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    print(\"4-bit quantization is enabled for GPU.\")\n",
        "\n",
        "base_model = GPT2LMHeadModel.from_pretrained(\n",
        "    GPT2_MODEL_NAME,\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "vit = ViTModel.from_pretrained(VIT_MODEL_NAME)\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. PEFT & LORA CONFIGURATION\n",
        "# ==============================================================================\n",
        "print(\"Configuring LoRA for both models...\")\n",
        "lora_config_gpt2 = LoraConfig(r=8, lora_alpha=16, target_modules=[\"c_attn\", \"c_proj\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM)\n",
        "lora_gpt2_model = get_peft_model(base_model, lora_config_gpt2)\n",
        "print(\"--- GPT-2 with LoRA ---\")\n",
        "lora_gpt2_model.print_trainable_parameters()\n",
        "\n",
        "lora_config_vit = LoraConfig(r=8, lora_alpha=16, target_modules=[\"query\", \"key\", \"value\", \"dense\"], lora_dropout=0.05, bias=\"none\")\n",
        "lora_vit_model = get_peft_model(vit, lora_config_vit)\n",
        "print(\"\\n--- ViT with LoRA ---\")\n",
        "lora_vit_model.print_trainable_parameters()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. DATASET AND DATALOADER (CORRECTED)\n",
        "# ==============================================================================\n",
        "print(\"\\nDownloading and preparing dataset...\")\n",
        "path = kagglehub.dataset_download(\"hsankesara/flickr-image-dataset\")\n",
        "csv_path = os.path.join(path, \"flickr30k_images/results.csv\")\n",
        "img_dir = os.path.join(path, \"flickr30k_images/flickr30k_images\")\n",
        "print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "df = pd.read_csv(csv_path, delimiter='|')\n",
        "df.columns = [col.strip() for col in df.columns]\n",
        "\n",
        "# <<<<<<<<<<<<<<<<<<<< THE FIX IS HERE >>>>>>>>>>>>>>>>>>>>\n",
        "# 1. Remove rows with missing values in the 'comment' column.\n",
        "df.dropna(subset=['comment'], inplace=True)\n",
        "\n",
        "# 2. Ensure all comments are strings, stripping any extra whitespace.\n",
        "df['comment'] = df['comment'].astype(str).str.strip()\n",
        "\n",
        "# 3. Reset the index of the DataFrame after dropping rows.\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "\n",
        "# --- Image Transformations ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# --- Custom PyTorch Dataset (No changes needed here) ---\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, tokenizer, transform=None, max_length=128):\n",
        "        self.dataframe = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            img_name = self.dataframe.loc[idx, 'image_name']\n",
        "            img_path = os.path.join(self.root_dir, img_name)\n",
        "            caption = self.dataframe.loc[idx, 'comment'] # This will now always be a string\n",
        "            image = PIL.Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            tokenized_caption = self.tokenizer(caption, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
        "            caption_ids = tokenized_caption['input_ids'].squeeze(0)\n",
        "            return image, caption_ids\n",
        "        except (PIL.UnidentifiedImageError, FileNotFoundError) as e:\n",
        "            print(f\"Warning: Skipping corrupted or missing image at index {idx}: {e}\")\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "dataset = FlickrDataset(df, img_dir, tokenizer, transform, MAX_TEXT_LENGTH)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. COMBINED MODEL ARCHITECTURE (Img2GPT)\n",
        "# ==============================================================================\n",
        "class Img2GPT(nn.Module):\n",
        "    def __init__(self, vit_model, gpt2_model):\n",
        "        super(Img2GPT, self).__init__()\n",
        "        self.vit = vit_model\n",
        "        self.gpt2 = gpt2_model\n",
        "        vit_dim = self.vit.config.hidden_size\n",
        "        gpt_dim = self.gpt2.config.hidden_size\n",
        "        self.proj = nn.Linear(vit_dim, gpt_dim)\n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
        "        image_embeds = vit_outputs.last_hidden_state\n",
        "        projected_image_embeds = self.proj(image_embeds)\n",
        "        if labels is not None:\n",
        "            text_embeds = self.gpt2.get_input_embeddings()(labels.long())\n",
        "            inputs_embeds = torch.cat([projected_image_embeds, text_embeds], dim=1)\n",
        "        else:\n",
        "            inputs_embeds = projected_image_embeds\n",
        "        output_labels = None\n",
        "        if labels is not None:\n",
        "            num_image_tokens = projected_image_embeds.shape[1]\n",
        "            mask_labels = torch.full(projected_image_embeds.shape[:-1], -100, device=labels.device)\n",
        "            output_labels = torch.cat([mask_labels, labels], dim=1)\n",
        "        outputs = self.gpt2(inputs_embeds=inputs_embeds, labels=output_labels)\n",
        "        return outputs\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_caption(self, pixel_values, max_length=50, num_beams=5):\n",
        "        self.eval()\n",
        "        pixel_values = pixel_values.to(next(self.parameters()).device)\n",
        "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
        "        image_embeds = vit_outputs.last_hidden_state\n",
        "        projected_image_embeds = self.proj(image_embeds)\n",
        "        generated_ids = self.gpt2.generate(\n",
        "            inputs_embeds=projected_image_embeds,\n",
        "            max_length=max_length + projected_image_embeds.shape[1],\n",
        "            num_beams=num_beams,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        captions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        return [caption.strip() for caption in captions]\n",
        "\n",
        "model = Img2GPT(lora_vit_model, lora_gpt2_model)\n",
        "model.to(device)\n",
        "\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total:,}\")\n",
        "print(f\"Trainable parameters: {trainable:,} ({100 * trainable / total:.2f}%)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. TRAINING LOOP WITH PERIODIC EVALUATION (CORRECTED)\n",
        "# ==============================================================================\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "use_amp = (DEVICE_CHOICE == \"cuda\")\n",
        "scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "# <<<<<<<<<<<<<<<<<<<< PREPARE FIXED BATCH FOR EVALUATION >>>>>>>>>>>>>>>>>>\n",
        "print(\"\\nFetching a fixed batch for periodic evaluation...\")\n",
        "# Set a temporary dataloader without shuffling to get a consistent batch\n",
        "temp_loader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
        "fixed_eval_batch = next(iter(temp_loader))\n",
        "eval_images, eval_captions_ids = fixed_eval_batch\n",
        "\n",
        "eval_images = eval_images.to(device)\n",
        "true_eval_captions = tokenizer.batch_decode(eval_captions_ids, skip_special_tokens=True)\n",
        "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "print(f\"\\nStarting training for {NUM_EPOCHS} epochs...\")\n",
        "print(f\"Automatic Mixed Precision (AMP) enabled: {use_amp}\")\n",
        "\n",
        "EVAL_INTERVAL = 1000 # Evaluate every 50 batches\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(enumerate(dataloader), desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", total=len(dataloader))\n",
        "\n",
        "    for batch_idx, (images, captions) in progress_bar:\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # --- Training Step ---\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(enabled=use_amp):\n",
        "            outputs = model(pixel_values=images, labels=captions)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        # --- Periodic Evaluation Step ---\n",
        "        if (batch_idx + 1) % EVAL_INTERVAL == 0:\n",
        "            print(f\"\\n--- Running evaluation at Epoch {epoch+1}, Batch {batch_idx+1} ---\")\n",
        "            torch.save(model.state_dict(), f\"img2gpt_epoch_{epoch+1}.pth\")\n",
        "            # <<<<<<<<<<<<<<<<<<<< THE FIX IS HERE >>>>>>>>>>>>>>>>>>>>\n",
        "            # The generation must also be inside an autocast context to handle float16/half dtypes\n",
        "            with autocast(enabled=use_amp):\n",
        "                generated_captions = model.generate_caption(eval_images, max_length=50)\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "            # Display results\n",
        "            for i in range(len(generated_captions)):\n",
        "                print(f\"  Image {i+1}:\")\n",
        "                print(f\"    -> True Caption: {true_eval_captions[i].strip()}\")\n",
        "                print(f\"    -> Generated Caption: {generated_captions[i]}\")\n",
        "\n",
        "            print(\"--- Evaluation finished, resuming training ---\")\n",
        "\n",
        "            # IMPORTANT: Switch back to training mode\n",
        "            model.train()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"\\nEpoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. FINAL INFERENCE EXAMPLE\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Running Final Inference Example on the evaluation batch ---\")\n",
        "# Generate final captions\n",
        "final_captions = model.generate_caption(eval_images, max_length=50)\n",
        "\n",
        "# Display results\n",
        "for i in range(len(final_captions)):\n",
        "    print(f\"\\n--- Image {i+1} ---\")\n",
        "    print(f\"  -> True Caption: {true_eval_captions[i].strip()}\")\n",
        "    print(f\"  -> Final Generated Caption: {final_captions[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "426b4a75be2f406ca149c2e12b27a7de",
            "3e091d61d96b4ac6b6f5acc0c335bcaf",
            "a05da501d48e426e8ba2d3ac671a8fe1",
            "cddcbb8a2fc8416b94e03c224ffc8f25",
            "dc24988b9fc347259069ef1bf6d4b9ed",
            "c92a3a9a0bde4ca98d67048020d75dd8",
            "ca91380e115743458624cbec68b746aa",
            "41f8e4154cd54f5287b3748d7d710bb3",
            "f2e4bea6ca6f4a2d9248489d04749c0a",
            "59c6b616d9b140c2818fbd669bc5fd62",
            "f504994f2fa04ef5a0f756053c116fd2",
            "067d5cc6bdba48869da5719fb0448e54",
            "3f55002a5e33412eacb80ccfd529dc39",
            "a877c6866d6943ab90ec612e4dd57155",
            "2a3200735fc94e1a8afa5e86b24dfa98",
            "6b2dc7f5852f42f4a285362e05f9472a",
            "472dbca603b94f71877be10d08ccf1a9",
            "58d238b69b0f4474baddf40c9de77327",
            "5d35d209f16841ea91668c0ec54bfc33",
            "16f1f2fc34c84b28a6e8ece967ad740e",
            "35fe8062bf394b9f93408b83ce2336c2",
            "66da745008984733a479ce0bb7907c75",
            "5f3afbac90ee4337b3d7e090af070e51",
            "9808c927ea7149d58c814a872cb3430d",
            "b0f4d314987e4e7aa8c23aecd477b4d8",
            "814c70e296324eac8987aaf148135327",
            "aa087b0f25194804824cb804d10d7a9f",
            "a65a2715c0914c1b85fd2b0830ae9604",
            "d0c1363c6c0c4810a09afdcd26fec5e8",
            "f70bd2f5621c41efaed46db6798b9aec",
            "4158e947107744a7aebcdd7e7f5a4ac3",
            "ffce5b6a3eb949c88b35d1a5d64969d6",
            "0648044715b7408e895135d7d690f248"
          ]
        },
        "id": "DrhDQup--wGk",
        "outputId": "20274707-dc2b-4dfa-de2d-1218e0abd115"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading models and tokenizer...\n",
            "Added [PAD] token to tokenizer.\n",
            "4-bit quantization is enabled for GPU.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA for both models...\n",
            "--- GPT-2 with LoRA ---\n",
            "trainable params: 811,008 || all params: 125,251,584 || trainable%: 0.6475\n",
            "\n",
            "--- ViT with LoRA ---\n",
            "trainable params: 1,339,392 || all params: 87,728,640 || trainable%: 1.5267\n",
            "\n",
            "Downloading and preparing dataset...\n",
            "Dataset downloaded to: /kaggle/input/flickr-image-dataset\n",
            "\n",
            "Total parameters: 171,103,488\n",
            "Trainable parameters: 2,740,992 (1.60%)\n",
            "\n",
            "Fetching a fixed batch for periodic evaluation...\n",
            "\n",
            "Starting training for 3 epochs...\n",
            "Automatic Mixed Precision (AMP) enabled: True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-253040945.py:203: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=use_amp)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "426b4a75be2f406ca149c2e12b27a7de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/3:   0%|          | 0/7946 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-253040945.py:232: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running evaluation at Epoch 1, Batch 1000 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-253040945.py:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man and a woman are walking down a street in the middle of the night in a dark colored van , looking at each other with their eyes closed , while the other man is standing in the middle of the street with his hands in his pockets .\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man and a woman are walking down a street in the middle of the night in a dark colored van , looking at each other with their eyes closed , while the other man is standing in the middle of the street with his hands in his pockets .\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man and a woman are walking down a street in the middle of the night in a dark colored van , looking at each other with their eyes closed , while the other man is standing in the middle of the street with his hands in his pockets .\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man and a woman are walking down a street in the middle of the night in a dark colored van , looking at each other with their eyes closed , while the other man is standing in the middle of the street with his hands in his pockets .\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 1, Batch 2000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt and a blue shirt is standing in front of a tree in the middle of the forest with a tree in front of him , while another man in a blue shirt is standing in front of a tree in the middle of the\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt and a blue shirt is standing in front of a tree in the middle of the forest with a tree in front of him , while another man in a blue shirt is standing in front of a tree in the middle of the\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt and a blue shirt is standing in front of a tree in the middle of the forest with a tree in front of him , while another man in a blue shirt is standing in front of a tree in the middle of the\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt and a blue shirt is standing in front of a tree in the middle of the forest with a tree in front of him , while another man in a blue shirt is standing in front of a tree in the middle of the\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 1, Batch 3000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a tree in front of a\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 1, Batch 4000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a white shirt and a black shirt is standing in front of a tree with a tree in front of him , while another man in a white shirt and a black shirt is standing in front of a tree with a tree in front of him\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a white shirt and a black shirt is standing in front of a tree with a tree in front of him , while another man in a white shirt and a black shirt is standing in front of a tree with a tree in front of him\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a white shirt and a black shirt is standing in front of a tree with a tree in front of him , while another man in a white shirt and a black shirt is standing in front of a tree with a tree in front of him\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a white shirt and a black shirt is standing in front of a tree with a tree in front of him , while another man in a white shirt and a black shirt is standing in front of a tree with a tree in front of him\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 1, Batch 5000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a white shirt and black pants is standing in front of a tree in a forested area with a man in a white shirt and black pants standing in front of a tree in a forested area with a man in a white shirt and\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a white shirt and black pants is standing in front of a tree in a forested area with a man in a white shirt and black pants standing in front of a tree in a forested area with a man in a white shirt and\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a white shirt and black pants is standing in front of a tree in a forested area with a man in a white shirt and black pants standing in front of a tree in a forested area with a man in a white shirt and\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a white shirt and black pants is standing in front of a tree in a forested area with a man in a white shirt and black pants standing in front of a tree in a forested area with a man in a white shirt and\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 1, Batch 6000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree with a man in a green shirt and a man in a blue shirt standing in front of a tree with a man in a blue shirt and a man in a green shirt\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree with a man in a green shirt and a man in a blue shirt standing in front of a tree with a man in a blue shirt and a man in a green shirt\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree with a man in a green shirt and a man in a blue shirt standing in front of a tree with a man in a blue shirt and a man in a green shirt\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree with a man in a green shirt and a man in a blue shirt standing in front of a tree with a man in a blue shirt and a man in a green shirt\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 1, Batch 7000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree with a tree in front of him and a man in a blue shirt and blue pants standing in front of a tree with a tree in front of him and a man in\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree with a tree in front of him and a man in a blue shirt and blue pants standing in front of a tree with a tree in front of him and a man in\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree with a tree in front of him and a man in a blue shirt and blue pants standing in front of a tree with a tree in front of him and a man in\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is standing in front of a tree with a tree in front of him and a man in a blue shirt and blue pants standing in front of a tree with a tree in front of him and a man in\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "Epoch 1 finished. Average Loss: 10.0481\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "067d5cc6bdba48869da5719fb0448e54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/3:   0%|          | 0/7946 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running evaluation at Epoch 2, Batch 1000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is playing a guitar in front of a tree in front of a house in front of a house in front of a house in front of a house in front of a house in front of a house in front\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is playing a guitar in front of a tree in front of a house in front of a house in front of a house in front of a house in front of a house in front of a house in front\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is playing a guitar in front of a tree in front of a house in front of a house in front of a house in front of a house in front of a house in front of a house in front\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt and blue pants is playing a guitar in front of a tree in front of a house in front of a house in front of a house in front of a house in front of a house in front of a house in front\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 2, Batch 2000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a white shirt and blue jeans is playing a guitar in a park with a man in a blue shirt and a woman in a white shirt and a man in a white shirt .BuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyable\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a white shirt and blue jeans is playing a guitar in a park with a man in a blue shirt and a woman in a white shirt and a man in a white shirt .BuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyable\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a white shirt and blue jeans is playing a guitar in a park with a man in a blue shirt and a woman in a white shirt and a man in a white shirt .BuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyable\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a white shirt and blue jeans is playing a guitar in a park with a man in a blue shirt and a woman in a white shirt and a man in a white shirt .BuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyableBuyable\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 2, Batch 3000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt and black pants is playing a guitar in a grassy area . convoluting his voice into a loud whistle that sounds like it is coming from inside his earphones while he is playing a guitar in front of a crowd .\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt and black pants is playing a guitar in a grassy area . convoluting his voice into a loud whistle that sounds like it is coming from inside his earphones while he is playing a guitar in front of a crowd .\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt and black pants is playing a guitar in a grassy area . convoluting his voice into a loud whistle that sounds like it is coming from inside his earphones while he is playing a guitar in front of a crowd .\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt and black pants is playing a guitar in a grassy area . convoluting his voice into a loud whistle that sounds like it is coming from inside his earphones while he is playing a guitar in front of a crowd .\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 2, Batch 4000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt and black pants is playing a guitar in a grassy area . convolvulus , which is located in the middle of a field of green grass , is surrounded by trees and is surrounded by a large group of people .\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt and black pants is playing a guitar in a grassy area . convolvulus , which is located in the middle of a field of green grass , is surrounded by trees and is surrounded by a large group of people .\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt and black pants is playing a guitar in a grassy area . convolvulus , which is located in the middle of a field of green grass , is surrounded by trees and is surrounded by a large group of people .\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt and black pants is playing a guitar in a grassy area . convolvulus , which is located in the middle of a field of green grass , is surrounded by trees and is surrounded by a large group of people .\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 2, Batch 5000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a building with a tree in the background , and a man in a green shirt is playing a guitar in front of a building with a tree in the background , and a man in a\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a building with a tree in the background , and a man in a green shirt is playing a guitar in front of a building with a tree in the background , and a man in a\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a building with a tree in the background , and a man in a green shirt is playing a guitar in front of a building with a tree in the background , and a man in a\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a building with a tree in the background , and a man in a green shirt is playing a guitar in front of a building with a tree in the background , and a man in a\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 2, Batch 6000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a violin . convolvulizes into one side of her face while another woman in a white shirt is playing the other side of her face while another woman\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a violin . convolvulizes into one side of her face while another woman in a white shirt is playing the other side of her face while another woman\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a violin . convolvulizes into one side of her face while another woman in a white shirt is playing the other side of her face while another woman\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a violin . convolvulizes into one side of her face while another woman in a white shirt is playing the other side of her face while another woman\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 2, Batch 7000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvulents into two groups of people standing in front of a tree with a man in a\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvulents into two groups of people standing in front of a tree with a man in a\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvulents into two groups of people standing in front of a tree with a man in a\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvulents into two groups of people standing in front of a tree with a man in a\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "Epoch 2 finished. Average Loss: 9.9622\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3/3:   0%|          | 0/7946 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f3afbac90ee4337b3d7e090af070e51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running evaluation at Epoch 3, Batch 1000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar on a grassy hillside with a man in a red shirt and a woman in a blue shirt . convolvents in front of a tree with a man in a blue shirt and a woman in\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar on a grassy hillside with a man in a red shirt and a woman in a blue shirt . convolvents in front of a tree with a man in a blue shirt and a woman in\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar on a grassy hillside with a man in a red shirt and a woman in a blue shirt . convolvents in front of a tree with a man in a blue shirt and a woman in\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar on a grassy hillside with a man in a red shirt and a woman in a blue shirt . convolvents in front of a tree with a man in a blue shirt and a woman in\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 3, Batch 2000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvents in front of a tree with a man in a blue shirt and a woman in a\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvents in front of a tree with a man in a blue shirt and a woman in a\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvents in front of a tree with a man in a blue shirt and a woman in a\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvents in front of a tree with a man in a blue shirt and a woman in a\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 3, Batch 3000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a building with a tree in the background and a man in a white shirt is playing a trumpet in front of a building with a\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a building with a tree in the background and a man in a white shirt is playing a trumpet in front of a building with a\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a building with a tree in the background and a man in a white shirt is playing a trumpet in front of a building with a\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a building with a tree in the background and a man in a white shirt is playing a trumpet in front of a building with a\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 3, Batch 4000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvents in front of a building with a man in a blue shirt and a woman in a\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvents in front of a building with a man in a blue shirt and a woman in a\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvents in front of a building with a man in a blue shirt and a woman in a\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar in front of a tree with a man in a red shirt and a woman in a white shirt . convolvents in front of a building with a man in a blue shirt and a woman in a\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 3, Batch 5000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a guitar in front of a building with a tree in the background and a man in a white shirt is playing a guitar in front of a building with a\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a guitar in front of a building with a tree in the background and a man in a white shirt is playing a guitar in front of a building with a\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a guitar in front of a building with a tree in the background and a man in a white shirt is playing a guitar in front of a building with a\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a guitar in front of a building with a tree in the background and a man in a white shirt is playing a guitar in front of a building with a\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 3, Batch 6000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a tree with a fence in the background and a man in a white shirt is playing a trumpet in front of a tree with a\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a tree with a fence in the background and a man in a white shirt is playing a trumpet in front of a tree with a\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a tree with a fence in the background and a man in a white shirt is playing a trumpet in front of a tree with a\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a tree with a fence in the background and a man in a white shirt is playing a trumpet in front of a tree with a\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "--- Running evaluation at Epoch 3, Batch 7000 ---\n",
            "  Image 1:\n",
            "    -> True Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a building with a tree in the background and a man in a white shirt is playing a trumpet in front of a building with a\n",
            "  Image 2:\n",
            "    -> True Caption: Two young , White males are outside near many bushes .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a building with a tree in the background and a man in a white shirt is playing a trumpet in front of a building with a\n",
            "  Image 3:\n",
            "    -> True Caption: Two men in green shirts are standing in a yard .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a building with a tree in the background and a man in a white shirt is playing a trumpet in front of a building with a\n",
            "  Image 4:\n",
            "    -> True Caption: A man in a blue shirt standing in a garden .\n",
            "    -> Generated Caption: A man in a blue shirt is playing a guitar while a woman in a white shirt is playing a trumpet in front of a building with a tree in the background and a man in a white shirt is playing a trumpet in front of a building with a\n",
            "--- Evaluation finished, resuming training ---\n",
            "\n",
            "Epoch 3 finished. Average Loss: 9.9428\n",
            "\n",
            "Training finished.\n",
            "\n",
            "--- Running Final Inference Example on the evaluation batch ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected scalar type Float but found Half",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-253040945.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Running Final Inference Example on the evaluation batch ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;31m# Generate final captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m \u001b[0mfinal_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;31m# Display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-253040945.py\u001b[0m in \u001b[0;36mgenerate_caption\u001b[0;34m(self, pixel_values, max_length, num_beams)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvit_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mprojected_image_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         generated_ids = self.gpt2.generate(\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojected_image_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprojected_image_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             )\n\u001b[1;32m   2635\u001b[0m             \u001b[0;31m# 12. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2637\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4079\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4081\u001b[0;31m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4083\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             outputs = block(\n\u001b[0m\u001b[1;32m    928\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0mpast_key_values\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, past_key_value, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m     ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]:\n\u001b[1;32m    409\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         attn_output, self_attn_weights = self.attn(\n\u001b[1;32m    412\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2903\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m         )\n\u001b[0;32m-> 2905\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2906\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Half"
          ]
        }
      ]
    }
  ]
}